{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Evaluation on Prostate Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to use trained Mask R-CNN on prostate dataset for evaluation. As for large pathology image, we crop each image to several patches. This notebook is designed to get the detection reulst for single patch and evaluate them use mIOU. You'd need a GPU, though, because the network backbone is a Resnet101, which would be slow to detect on a CPU.\n",
    "\n",
    "The code of the Prostate dataset can be found in prostate.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import module from system lib\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import module from maskrcnn repo\n",
    "from config import Config\n",
    "import utils\n",
    "import model as modellib\n",
    "import visualize\n",
    "from model import log\n",
    "import prostate\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Specify GPU to use\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE_SHAPES                [[128 128]\n",
      " [ 64  64]\n",
      " [ 32  32]\n",
      " [ 16  16]\n",
      " [  8   8]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [ 0.1  0.1  0.2  0.2]\n",
      "DETECTION_CROP                 [128, 384, 128, 384]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.5\n",
      "DETECTION_NMS_THRESHOLD        1\n",
      "GPU_COUNT                      2\n",
      "IMAGES_PER_GPU                 4\n",
      "IMAGE_MAX_DIM                  512\n",
      "IMAGE_MIN_DIM                  512\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [512 512   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [ 193.95062641  120.83140887  183.74016669]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "MODE                           16\n",
      "NAME                           prostate\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [ 0.1  0.1  0.2  0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "USE_TUMORCLASS                 False\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify the dir that store the prostate dataset\n",
    "dataset_dir = os.path.join(os.path.dirname(os.getcwd()), \"Data_Pre_Processing/cedars-224\")\n",
    "# We do 5-fold validation, specify which fold to be exclude for the current run\n",
    "held_out_set = 0\n",
    "# Featch the mean_pixel based on the training data (data exclude the held_out_set)\n",
    "mean_pixel = prostate.Mean_pixel(dataset_dir, held_out_set)\n",
    "# Configuration\n",
    "class EvaluationConfig(prostate.ProstateConfig):\n",
    "    GPU_COUNT = 2\n",
    "    IMAGES_PER_GPU = 4\n",
    "    DETECTION_MIN_CONFIDENCE = 0.5\n",
    "    DETECTION_NMS_THRESHOLD = 1     \n",
    "    MEAN_PIXEL = np.array(mean_pixel)\n",
    "    IMAGE_MAX_DIM = 512\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    DETECTION_CROP = [128, 384, 128, 384] # [height_crop_start, height_crop_end, width_crop_start, width_crop_end]\n",
    "    MODE = 16\n",
    "evaluation_config = EvaluationConfig()\n",
    "evaluation_config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Graph and Loading Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from  /scratch/wenyuan/Mask_RCNN_On_Pathology/Mask_RCNN/logs/prostate20180322T1029_held_out_set_0/mask_rcnn_prostate_0071.h5\n"
     ]
    }
   ],
   "source": [
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"detection\", \n",
    "                          config=evaluation_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "# Get path to saved weights\n",
    "# Either set a specific path, find a trained weights specified by epoch and held_out_set or find last trained weights\n",
    "h5_filename = None # Specify the h5 filename here if you want to choose a specific file\n",
    "epoch = 71\n",
    "\n",
    "if h5_filename is not None:\n",
    "    model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "elif epoch == -1:    \n",
    "    model_path = model.find_last()[1]\n",
    "else:\n",
    "    try:\n",
    "        model_path = model.find_specific(epoch = epoch, held_out_set = held_out_set)[1]\n",
    "    except:\n",
    "        model_path = model.find_specific(epoch = epoch)[1]\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset and Specify the Interested Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_val = prostate.ProstateDataset()\n",
    "_, val_list = dataset_val.generator_patition(dataset_dir, held_out_set)\n",
    "# val_list = [image for image in val_list if image not in exclude_list]\n",
    "dataset_val.load_prostate(dataset_dir, val_list, mode = 16)\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "len(images) must be equal to BATCH_SIZE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8b595dfd0af0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mgt_sementic_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_sementic_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ATmask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# crop the label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Run object detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m# TODO: create a function for evaluation in model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/wenyuan/Mask_RCNN_On_Pathology/Mask_RCNN/model.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(self, images, verbose)\u001b[0m\n\u001b[1;32m   2540\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"detection\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Create model in detection mode.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m         assert len(\n\u001b[0;32m-> 2542\u001b[0;31m             images) == self.config.BATCH_SIZE, \"len(images) must be equal to BATCH_SIZE\"\n\u001b[0m\u001b[1;32m   2543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: len(images) must be equal to BATCH_SIZE"
     ]
    }
   ],
   "source": [
    "# Initialize the confusion matrix\n",
    "C_MATRIX = np.zeros((4, 4))\n",
    "# Create crop region\n",
    "hv, wv = utils.create_crop_region(evaluation_config) # meshgrid for crop region\n",
    "# Process display setting\n",
    "display_step = 10 # print out process for every display_step images\n",
    "total_image = len(val_list)\n",
    "for image_id in dataset_val.image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "            modellib.load_image_gt(dataset_val, evaluation_config,\n",
    "                                   image_id, use_mini_mask=False)\n",
    "    # Convert gt-instance mask to gt-sementic mask\n",
    "    gt_sementic_mask = utils.instance_2_sementic(gt_mask, gt_class_id)\n",
    "    gt_sementic_mask = gt_sementic_mask['ATmask'][hv, wv] # crop the label\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    # TODO: create a function for evaluation in model\n",
    "    r = results[0]    \n",
    "    if np.argmax(r['tumor_probs']) == 1:\n",
    "        det_sementic_mask = r['sementic_mask']\n",
    "    else:\n",
    "        det_sementic_mask = np.zeros((image.shape[0], image.shape[1]))\n",
    "    det_sementic_mask = det_sementic_mask[hv, wv] # crop the detection    \n",
    "    # Compute confusion matrix\n",
    "    c_matrix = confusion_matrix(np.ravel(gt_sementic_mask), np.ravel(det_sementic_mask))\n",
    "    # Expand the c_matrix to NUM_CLASSES * NUM_CLASSES\n",
    "    c_matrix = utils.expand_c_matrix(c_matrix, evaluation_config.NUM_CLASSES, gt_sementic_mask, det_sementic_mask)    \n",
    "    # Update cofusion matrix\n",
    "    C_MATRIX = C_MATRIX + c_matrix\n",
    "    # Display the process\n",
    "    if ((image_id + 1) % (evaluation_config.MODE * display_step) == 0):\n",
    "        print('Done evaluating %d / %d!'%((image_id + 1) / evaluation_config.MODE, total_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mIOU, IOU, below_th = utils.compute_mIOU(C_MATRIX, th = 0.5)\n",
    "print('Confusion Matrix:\\n', C_MATRIX)\n",
    "print(' mIOU:', mIOU, '\\n', \n",
    "      'IOU for each class:', IOU, '\\n',\n",
    "      'Below_th:', below_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
